# AI_X_DeepLearning  

> 1. Q-Learning


> 2. Deep RL  
> 모델 프리 상황에 상태 공간(state space)과 액션 공간(action space)이 매우 커서 밸류를 일일이 테이블에 담지 못하는 상황에서는 본격적으로 뉴럴넷과 강화 학습이 접목한다.  
 가치 기반 에이전트는 가치 함수에 근거하여 액션을 선택한다. 모델 프리 상황에서는 v(s)만 가지고 액션을 정할 수 없기 때문에 가치 기반 에이전트는 q(s,a)를 필요로 한다. 상태 s에서 선택할 수 있는 액션들 중에서 가장 밸류가 높은 액션을 선택하는 방식이다. Q러닝의 에이전트는 대표적인 가치 기반 에이전트로, Q러닝을 큰 문제로 확장하여 뉴럴넷을 이용해 최적의 정책을 찾을 수 있다.  
 앞서 보았던 Q러닝은 벨만 최적방정식을 이용해 최적 액션-벨류인〖 Q〗_* (s,a)를 학습하는 것이다. 이때 〖 Q〗_* (s,a)는 테이블에 적혀 있는 값으로, 아래와 같은 테이블 업데이트 수식을 갖고 있었다.   
Q(s,\ a)=\ Q(s,\ a)+\alpha(r+\ \gamma\ \ max\below(a^\prime\ )\funcapply Q(s^\prime,\ a^\prime\ )-Q(s,\ a))   
딥 Q러닝은 Q러닝을 뉴럴넷으로 확장하는 것이다. 테이블 업데이트 식을 보면 정답인 r+ γ  max┬(a^' )⁡Q(s^',a^' )와 현재 추측치인 Q(s,a) 사이 차이를 줄이는 방향으로 업데이트 한다. 뉴럴넷에서도 마찬가지 방식으로 손실 함수를 정의할 수 있다. r+ γ  max┬(a^' )⁡Q(s^',a^' )를 정답으로 보고, 이것과 〖 Q〗_θ (s,a) 사이 차이의 제곱을 손실 함수라 정의하겠다.  
L(θ)= E[〖(r+ γ  max┬(a^' )⁡Q(s^',a^' )-Q(s,a))〗^2 ]  
손실 함수를 정의할 때에는 기댓값 연산자 E가 반드시 필요하다. 같은 상태 s에서 같은 액션 a를 선택한다 하더라도 매번 다른 상태에 도달할 수 있기 때문이다. 물론 실제로 뉴럴넷을 업데이트할 때는 샘플 기반 방법론으로 E를 무시하고 계산할 수 있다. 데이터를 여러 개 모아서 그 평균을 이용해 업데이트하는 것이 그 방법이다. 이런 방식으로 하나의 데이터에 대해 θ를 업데이트하는 식을 적어보면 다음과 같다.  
θ^'= θ+α(r+ γ  max┬(a^' )⁡〖Q_θ (s^',a^' )〗-Q_θ (s,a))∇_θ Q_θ (s,a)    
이 식을 이용하여 θ를 계속해서 업데이트해 나가면 Q_θ (s,a)는 점점 최적의 액션-가치 함수 Q_* (s,a)에 가까워질 것이다.   

 


> 3. DQN


> 4. Example
